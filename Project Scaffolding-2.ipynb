{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4212b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install  fastparquet==0.7.1\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import geopy.distance\n",
    "import shapely\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "weather_2009_CSV=\"2009_weather.csv\"\n",
    "weather_2010_CSV=\"2010_weather.csv\"\n",
    "weather_2011_CSV=\"2011_weather.csv\"\n",
    "weather_2012_CSV=\"2012_weather.csv\"\n",
    "weather_2013_CSV=\"2013_weather.csv\"\n",
    "weather_2014_CSV=\"2014_weather.csv\"\n",
    "\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "For uber dataset, since there is no \"distance\" column, we need to calculate distance based on pickup's and dropoff's longitude and latitude. We use the euclidean distance between start and end coordinate. In order to keep datasets consistant, we ignore the \"distance\" column in yellow taxi dataset and calculate new \"distance\" column (using the same process as uber dataset does)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2730f",
   "metadata": {},
   "source": [
    "The function calculate_distance(picklon, picklat,droplon,droplat) will take pickup's and dropoff's longitudes and lantitudes, and then return the euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(picklon, picklat,droplon,droplat):\n",
    "    #return the euclidean distance between start and end coordinate\n",
    "    from math import sin, cos, sqrt, atan2, radians\n",
    "    R = 6373.0 #approximate radius of earth in km\n",
    "    lon1=picklon\n",
    "    lat1=picklat\n",
    "    lon2=droplon\n",
    "    lat2=droplat\n",
    "    dlon=lon2-lon1\n",
    "    dlat=lat2-lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    b = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance=R*b\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a73bc9",
   "metadata": {},
   "source": [
    "Function get_lon_lat(dataframe) is used to deal with taxi datasets, since some taxi datasets only contain location ID. We need to convert ID to latitude and longitude.get_lon_lat(dataframe) will return dataframe with four new columns \"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",and \"dropoff_latitude\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be67edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lon_lat(dataframe):\n",
    "    zone_file=gpd.read_file(\"taxi_zones\")\n",
    "    zone_file['geometry']=zone_file['geometry'].to_crs(4326)\n",
    "    zone_file[\"lon\"]=zone_file['geometry'].centroid.x\n",
    "    zone_file[\"lat\"]=zone_file['geometry'].centroid.y\n",
    "    pick=zone_file[[\"LocationID\",\"lon\",\"lat\"]]\n",
    "    #rename columns\n",
    "    pick.columns=[\"PULocationID\",\"pickup_longitude\",\"pickup_latitude\"]\n",
    "    drop=zone_file[[\"LocationID\",\"lon\",\"lat\"]]\n",
    "    #rename columns\n",
    "    drop.columns=[\"DOLocationID\",\"dropoff_longitude\",\"dropoff_latitude\"]\n",
    "    #merge pick and drop to dataframe\n",
    "    #merge pick \n",
    "    dataframe=pd.merge(left=dataframe,right=pick)\n",
    "    #merge drop\n",
    "    dataframe=pd.merge(left=dataframe,right=drop)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad025f7d",
   "metadata": {},
   "source": [
    "Function add_distance_column(dataframe) will add new column \"distance\" through applying function calculate_distance to each row of the dataframe, which using columns \"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",and \"dropoff_latitude\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    dataframe[\"distance\"]=dataframe.apply(lambda x: calculate_distance(x[\"pickup_longitude\"],x[\"pickup_latitude\"],\n",
    "                                                             x[\"dropoff_longitude\"],x[\"dropoff_latitude\"]),axis=1)\n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484705e6",
   "metadata": {},
   "source": [
    "After exploring yellow taxi datasets, we find that dataset's columns, column names, datatype are different among different year. Thus, we need to clean taxi data for following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177e9d2",
   "metadata": {},
   "source": [
    "First, we write a function called find_taxi_Parquet_urls, which will return urls for Parquet dataset from January 2009 through June 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_Parquet_urls():\n",
    "    url=TAXI_URL #url for taxi data page is set above\n",
    "    url_list=list()\n",
    "    response=requests.get(url)\n",
    "    if response.status_code!=200:\n",
    "        print(\"Failure: fail to request data\")\n",
    "        return result\n",
    "    try:\n",
    "        result_page=BeautifulSoup(response.content,'lxml')\n",
    "        result_tags=result_page.find_all(\"a\",{\"title\":\"Yellow Taxi Trip Records\"})\n",
    "        for element in result_tags:\n",
    "            newurl=element.get('href')\n",
    "            url_list.append(newurl)\n",
    "        #only include dataset link from January 2009 through June 2015 \n",
    "        result=url_list[80:86]\n",
    "        result.extend(url_list[92:]) \n",
    "    except:\n",
    "        print(\"Failure: scraping fail\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1629a68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2016-12.parquet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of using find_taxi_Parquet_urls()\n",
    "result=find_taxi_Parquet_urls()\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a56a5",
   "metadata": {},
   "source": [
    "Function get_and_clean_month_taxi_data(url) will take one url for taxi parque as input. Then, it will normalize column name, normalize data type, drop null value, remove data point outside the coordinate box, and sampling the data based on uber dataset's size. The output of the function will be a cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "def get_and_clean_month_taxi_data(url):\n",
    "    #get month Taxi data\n",
    "    df=pd.read_parquet(url,engine='fastparquet')\n",
    "    \n",
    "    #possible column name after looking at each dataset\n",
    "     #list1=['tpep_pickup_datetime',\"trip_distance\",\"PULocationID\",\"DOLocationID\",\"tip_amount\"]\n",
    "     #list2=['pickup_datetime','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','tip_amount']\n",
    "     #list3=['Trip_Pickup_DateTime','Trip_Distance','Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat','Tip_Amt']\n",
    "    \n",
    "    #deal with df.column contains list1\n",
    "    if df.columns.str.contains('tpep_pickup_datetime').any():#df.column contain list1         \n",
    "        #select column\n",
    "        df=df[[\"tpep_pickup_datetime\",\"PULocationID\",\"DOLocationID\",\"tip_amount\"]]\n",
    "        #From taxi_zone, we know that loction ID is valid between 1 and 263. Thus, we can use it to clean our taxi dataset.\n",
    "        #remove invalid locationIDï¼šlocation ID should between 1 and 263, and should be numeric\n",
    "        df=df.loc[(df[\"PULocationID\"]>=1) & (df[\"PULocationID\"]<=263) & (df[\"DOLocationID\"]>=1)\n",
    "                & (df[\"DOLocationID\"]<=263)]\n",
    "       \n",
    "        #only select pickup id not equal to drop off id\n",
    "        df=df.loc[df[\"PULocationID\"]!=df[\"DOLocationID\"]]\n",
    "        #add longitude and latitude based on location ID\n",
    "        df=get_lon_lat(df)\n",
    "        #delete id\n",
    "        del df[\"DOLocationID\"]\n",
    "        del df[\"PULocationID\"]\n",
    "        #rename 'tpep_pickup_datetime'\n",
    "        df.rename(columns={\"tpep_pickup_datetime\": \"pickup_datetime\"}, inplace=True)\n",
    "        \n",
    "    #deal with df.column contains list2\n",
    "    elif df.columns.str.contains('pickup_datetime').any():\n",
    "        df=df[['pickup_datetime','tip_amount','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "        #change pickup_datetime to datetime object \n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        \n",
    "    #deal with df.column contains list3    \n",
    "    elif df.columns.str.contains('Trip_Pickup_DateTime').any():\n",
    "        df=df[['Trip_Pickup_DateTime','Tip_Amt','Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat']]\n",
    "        df.rename(columns={\"Trip_Pickup_DateTime\": \"pickup_datetime\",'Start_Lon':'pickup_longitude', \n",
    "                           'Start_Lat':'pickup_latitude', 'End_Lon':'dropoff_longitude', \n",
    "                           'End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'}, inplace=True)\n",
    "        #change pickup_datetime to datetime object \n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        \n",
    "    #remove outside the latitude/longitude coordinate box\n",
    "    df=df.loc[(df[\"pickup_longitude\"]<=-73.717047) & (df[\"pickup_longitude\"]>=-74.242330) \n",
    "    & (df[\"pickup_latitude\"]<=40.908524) & (df[\"pickup_latitude\"]>=40.560445)\n",
    "    & (df[\"dropoff_longitude\"]<=-73.717047) & (df[\"dropoff_longitude\"]>=-74.242330)\n",
    "    & (df[\"dropoff_latitude\"]<=40.908524)  & (df[\"dropoff_latitude\"]>=40.560445)]\n",
    "\n",
    "    #remove any row with nan value\n",
    "    df.dropna(inplace=True)\n",
    "    #sample dataframe, since for uber dataset,roughly 2500 data per month.\n",
    "    df=df.sample(n=2500,random_state = 1,ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7c5ff",
   "metadata": {},
   "source": [
    "Function get_and_clean_taxi_data() will first find all url links. Then, each url link will be feed to function get_and_clean_month_taxi_data() to get a cleaned dataframe and add \"distance\" column to it. Finally, it concats all dataframes and return it as one gigantic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_Parquet_urls = find_taxi_Parquet_urls()\n",
    "    if os.path.exists('taxi_data.csv'):\n",
    "        taxi_data=pd.read_csv('taxi_data.csv')\n",
    "    else:        \n",
    "        for Parquet_url in all_Parquet_urls:\n",
    "            dataframe = get_and_clean_month_taxi_data(Parquet_url)\n",
    "            #add distance column \n",
    "            dataframe=add_distance_column(dataframe)\n",
    "            all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "        # create one gigantic dataframe with data from every month needed\n",
    "        taxi_data = pd.concat(all_taxi_dataframes)\n",
    "        #save file\n",
    "        taxi_data.to_csv('taxi_data.csv',index=False)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e1902",
   "metadata": {},
   "source": [
    "After exploring uber dataset, we notices that we need to change the column type for \"pickuo_datatime\" from object to dataobject. And then selected appropriate columns for futher analysis. We also explore the Uber dataset for each month. Uber datset contains around 2500 rows for each month. Thus, we use this number to sample yellow taxi dataset, which ensure that yellow taxi dataset contain rougly equal number of sample as the uber dataset does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc736d",
   "metadata": {},
   "source": [
    "Function load_and_clean_uber_data(csv_file) takes the csv file name as an input. Then it only chooses columns \"pickup_datetime\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\", and \"dropoff_latitude\" from the original dataset. After that, it normalize data type of 'pickup_datetime'. Finally, it will return a cleaned Uber dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df=pd.read_csv(csv_file)\n",
    "    #clean data\n",
    "    #pick column\n",
    "    df=df[[\"pickup_datetime\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]]\n",
    "    #column type\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']).dt.tz_convert(None)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a45a",
   "metadata": {},
   "source": [
    "Function get_uber_data() will return a cleaned dataset with one new column \"distance\" (using function add_distance_column()). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    if os.path.exists('uber_data.csv'):\n",
    "        uber_dataframe=pd.read_csv('uber_data.csv')\n",
    "    else:\n",
    "        uber_dataframe = load_and_clean_uber_data(\"UBER_CSV\")\n",
    "        #add distance column\n",
    "        uber_dataframe=add_distance_column(uber_dataframe)\n",
    "        uber_dataframe=uber_dataframe.dropna()\n",
    "        #store\n",
    "        uber_dataframe.to_csv('uber_data.csv',index=False)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abfc35",
   "metadata": {},
   "source": [
    "By exploratory analysis and reading the description of dataset \"weather\", we found only rainfall of a day exceeds a threshold that can be measured, the filed hourly precipitation does contain a numeric value. Otherwise, it will be empty or a character \"T\". Hence, the first step is to replace the charater \"T\" to empty, and then tranform data type of hourly precipitation and hourly wind speed to number. After that, we convert data type of field date to datetime and drop all the all-null row in dataframe.\n",
    "\n",
    "Function clean_month_weather_data_hourly(csv_file) takes the csv file name as an input. Then it only chooses columns \"DATE\",\"HourlyWindSpeed\", and \"HourlyPrecipitation\" from the original dataset. After that, it clear the unrecognized value in HourlyPrecipitation and normalize data types of 'HourlyPrecipitation', 'HourlyWindSpeed' and 'DATE'. Finally, it will return a cleaned weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # only select columns will be used\n",
    "    selected = df[['DATE', 'HourlyWindSpeed', 'HourlyPrecipitation']]\n",
    "    # convert data type to number\n",
    "    selected['HourlyPrecipitation'] = pd.to_numeric(selected['HourlyPrecipitation'].replace(['T'], ''), errors='coerce')\n",
    "    selected['HourlyWindSpeed'] = pd.to_numeric(selected['HourlyWindSpeed'].replace(['T'], ''), errors='coerce')\n",
    "    selected['DATE'] = pd.to_datetime(selected['DATE'])\n",
    "    # filter na data\n",
    "    selected = selected.dropna()\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591bc0e",
   "metadata": {},
   "source": [
    "Function clean_month_weather_data_daily(csv_file) takes the csv file name as an input. Then it only chooses columns \"DATE\", \"HourlyWindSpeed\", and \"HourlyPrecipitation\" from the original dataset. After that, it clears the unrecognized value in HourlyPrecipitation and normalize data types of 'HourlyPrecipitation', 'HourlyWindSpeed' and 'DATE'. Finally, it creates two new columns with mean value of hourly wind speed and sum value of hourly percipitation, and then return a cleaned daily weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # only select columns will be used\n",
    "    selected = df[['HourlyWindSpeed', 'HourlyPrecipitation', 'DATE']]\n",
    "    # convert data type to number\n",
    "    selected['HourlyPrecipitation'] = pd.to_numeric(selected['HourlyPrecipitation'].replace(['T'], ''), errors='coerce')\n",
    "    selected['HourlyWindSpeed'] = pd.to_numeric(selected['HourlyWindSpeed'].replace(['T'], ''), errors='coerce')\n",
    "    # filter na data\n",
    "    selected = selected.dropna()\n",
    "    # convert data time to date string\n",
    "    selected['DATE'] = pd.to_datetime(selected['DATE']).dt.strftime('%Y-%m-%d')\n",
    "    # get average wind speed and toal percipitation of each day\n",
    "    daily_df = selected.groupby(['DATE']).agg({'HourlyWindSpeed': ['mean'], 'HourlyPrecipitation': ['sum']})\n",
    "    # rename df\n",
    "    daily_df.rename(columns = {'HourlyWindSpeed':'DailyWindSpeed', 'HourlyPrecipitation':'DailyPrecipitation'}, inplace = True)\n",
    "    return daily_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12281b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3f0e1e8",
   "metadata": {},
   "source": [
    "Function load_and_clean_weather_data() find all weather-like csv files in specified directory. For each weather file, it uses clean_month_weather_data_hourly and clean_month_weather_data_daily functions to clean data and append cleaned reult into two lists respectively. After that, it merges and returns all the cleaned dataset in each list to two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # read all weather-like files from given path\n",
    "    import glob\n",
    "    weather_csv_files = list(glob.glob('./*weather*.csv'))\n",
    "    weather_csv_files.sort()\n",
    "\n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591ee11",
   "metadata": {},
   "source": [
    "Call the data process function to clean taxi trip data, uber trip data, hourly weather data and daily weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['HourlyWindSpeed', 'HourlyPrecipitation'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kh/jc_vc4mj42s708qkj0c26b6w0000gn/T/ipykernel_28184/819989297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtaxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muber_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_uber_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhourly_weather_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_weather_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_weather_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/kh/jc_vc4mj42s708qkj0c26b6w0000gn/T/ipykernel_28184/3979875541.py\u001b[0m in \u001b[0;36mload_and_clean_weather_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweather_csv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhourly_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_month_weather_data_hourly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdaily_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_month_weather_data_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhourly_dataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhourly_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kh/jc_vc4mj42s708qkj0c26b6w0000gn/T/ipykernel_28184/3493153493.py\u001b[0m in \u001b[0;36mclean_month_weather_data_hourly\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# only select columns will be used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HourlyWindSpeed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HourlyPrecipitation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# convert data type to number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HourlyPrecipitation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HourlyPrecipitation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5794\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5798\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5858\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5859\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5861\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['HourlyWindSpeed', 'HourlyPrecipitation'] not in index\""
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c71c34",
   "metadata": {},
   "source": [
    "Based on sqlalchemy, this block is designed to connect sqlite service at first and then create tables with schema sqls, which is prepared based on the data type of all dataframes we cleaned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a db instance to do operation in db\n",
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE hourly_weather(\n",
    "   DATE timestamp PRIMARY KEY,\n",
    "   HourlyWindSpeed FLOAT,\n",
    "   HourlyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE daily_weather(\n",
    "   DATE timestamp PRIMARY KEY,\n",
    "   DailyWindSpeed FLOAT,\n",
    "   DailyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE taxis_trips(\n",
    "   pickup_datetime timestamp,\n",
    "   tip_amount FLOAT,\n",
    "   pickup_longitude DOUBLE,\n",
    "   pickup_latitude DOUBLE,\n",
    "   dropoff_longitude DOUBLE,\n",
    "   dropoff_latitude DOUBLE,\n",
    "   distance DOUBLE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE uber_trips(\n",
    "   pickup_datetime timestamp,\n",
    "   pickup_longitude DOUBLE,\n",
    "   pickup_latitude DOUBLE,\n",
    "   dropoff_longitude DOUBLE,\n",
    "   dropoff_latitude DOUBLE,\n",
    "   distance DOUBLE\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c344b",
   "metadata": {},
   "source": [
    "Function write_dataframes_to_table(table_to_df_dict) first convert all dataframe to sqls and then export to sqlite instance table by table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to sql and store data to db\n",
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(name=table_name, con=engine.connect(), index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map between db table and dataframe\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sql to file at specified direction\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(QUERY_DIRECTORY + '/' + str(outfile), \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c70b1",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "The query below first filter the taxi trips records between '2009-01-01' and '2015-06-30', and then count the records group by hour of pickup time of trip, and the result is ordered by hourly trip popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eee64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "select strftime('%H', pickup_datetime) as hour_of_day, count(1) as popularity\n",
    "from taxi_trips\n",
    "where strftime('%s', pickup_datetime) BETWEEN strftime('%s', '2009-01-01') AND strftime('%s', '2015-06-30')\n",
    "group by strftime('%H', pickup_datetime)\n",
    "order by popularity desc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b44d1",
   "metadata": {},
   "source": [
    "The result shows 19:00 PM is most popular hour for yellow taxi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86897c",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "The query below first filter the Uber trips records between '2009-01-01' and '2015-06-30', and then count the records group by day of pickup time of trip, and the result is ordered by daily trip popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31351df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "select count(1) as popularity, strftime('%w', pickup_datetime) as day_of_week\n",
    "from uber_trips\n",
    "where strftime('%s', pickup_datetime) BETWEEN strftime('%s', '2009-01-01') AND strftime('%s', '2015-06-30')\n",
    "group by strftime('%w', pickup_datetime)\n",
    "order by popularity desc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd620d",
   "metadata": {},
   "source": [
    "The result shows Friday is most popular weekday for uber."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f5803",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "The query below first union all taxis trips and uber trips. Secondly, it uses window function to sort all trips by distance and the rank will be add to result table. The final step is to find the distance value at or most near to the percentile 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "with all_trips as (\n",
    "    SELECT distance,\n",
    "        pickup_datetime\n",
    "    FROM taxi_trips\n",
    "    where strftime('%Y-%m', pickup_datetime) == \"2013-07\"\n",
    "    union all\n",
    "    SELECT distance,\n",
    "        pickup_datetime\n",
    "    FROM uber_trips\n",
    "    where strftime('%Y-%m', pickup_datetime) == \"2013-07\"\n",
    "),\n",
    "sorted_trips as (\n",
    "    SELECT distance,\n",
    "        pickup_datetime,\n",
    "        ROW_NUMBER() OVER (ORDER BY distance DESC) AS rank\n",
    "    FROM all_trips\n",
    ")\n",
    "select distance from sorted_trips\n",
    "where rank = (select ROUND((count(1) - 1) * (1 - 0.95)) from sorted_trips where strftime('%Y-%M', pickup_datetime) == \"2013-07\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11867d46",
   "metadata": {},
   "source": [
    "The trip distance that most near to the percentile 0.95 is 10621.171443418036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f2b20",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "The query below first union all taxis trips and uber trips, and then calculate total trip count and average distance of each day in year 2009. Only the 10 days with top 10 trip popularity will be retrieved as result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4= \"\"\"\n",
    "with all_trips as (\n",
    "    SELECT distance,\n",
    "        pickup_datetime\n",
    "    FROM taxi_trips\n",
    "    where strftime('%Y', pickup_datetime) = '2009'\n",
    "    union all\n",
    "    SELECT distance,\n",
    "        pickup_datetime\n",
    "    FROM uber_trips\n",
    "    where strftime('%Y', pickup_datetime) = '2009'\n",
    ")\n",
    "select strftime('%Y-%m-%d', pickup_datetime), count(1) as popularity, avg(distance) as avg_dist\n",
    "from all_trips\n",
    "group by strftime('%Y-%m-%d', pickup_datetime)\n",
    "order by popularity desc\n",
    "limit 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce51c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8717b",
   "metadata": {},
   "source": [
    "The result above shows the days with the highest number of hired rides for 2009\n",
    "and the corressponding trips count, average distance that having highest trip popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a5262",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "The query below first union all taxis trips and uber trips, and then left join the daily weather data to get daily wind speed and trip count of each day in year 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbe92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5= \"\"\"\n",
    "with weather as (\n",
    "    select date, DailyWindSpeed from daily_weather \n",
    "    where strftime('%Y', date) = '2014'\n",
    "    order by DailyWindSpeed desc\n",
    "    limit 10\n",
    "),\n",
    "all_trip as (\n",
    "    select pickup_datetime from taxi_trips\n",
    "    where strftime('%Y', pickup_datetime) = '2014'\n",
    "    union all\n",
    "    select pickup_datetime from uber_trips\n",
    "    where strftime('%Y', pickup_datetime) = '2014'\n",
    ")\n",
    "select date, max(DailyWindSpeed), count(1) as popularity\n",
    "from weather\n",
    "left join all_trip\n",
    "on weather.date = strftime('%Y-%m-%d', all_trip.pickup_datetime)\n",
    "group by weather.date\n",
    "order by date asc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d84952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72b21e",
   "metadata": {},
   "source": [
    "The result above shows the days were the windiest, and these days' trips count, average distance that having highest trip popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d289e45",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "The query below first union all taxis trips and uber trips, and then calculate total trip count of each day and hour. Secondly, the trip count result left join hourly weather data to get the hourly wind speed and hourly precipitation of each day and hour in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6= \"\"\"\n",
    "with all_trip as (\n",
    "    select pickup_datetime from taxi_trips\n",
    "    where strftime('%s', pickup_datetime) BETWEEN strftime('%s', '2012-10-22') AND strftime('%s', '2012-10-30')\n",
    "    union all\n",
    "    select pickup_datetime from uber_trips\n",
    "    where strftime('%s', pickup_datetime) BETWEEN strftime('%s', '2012-10-22') AND strftime('%s', '2012-10-30')\n",
    "),\n",
    "trip_aggr as (\n",
    "    select strftime('%Y-%m-%d:%H', pickup_datetime) as day_hour, count(1) as heat\n",
    "    from all_trip\n",
    "    group by strftime('%Y-%m-%d:%H', pickup_datetime)\n",
    ")\n",
    "select day_hour, heat, coalesce(HourlyWindSpeed, 0), coalesce(HourlyPrecipitation,0)\n",
    "from trip_aggr\n",
    "left join hourly_weather\n",
    "on trip_aggr.day_hour = strftime('%Y-%m-%d:%H', hourly_weather.date)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrieval result of given sql\n",
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada4c99",
   "metadata": {},
   "source": [
    "The result above shows the days during Hurricane Sandy in NYC and the week leading up to it, and the trips were taken each hour as well the hourly wind speed and precipitation of these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the sqls as files\n",
    "write_query_to_file(QUERY_1, \"taxi_hourly_trip.sql\")\n",
    "write_query_to_file(QUERY_2, \"uber_daily_trip.sql\")\n",
    "write_query_to_file(QUERY_3, \"upper_trip_distance.sql\")\n",
    "write_query_to_file(QUERY_4, \"2009_peak_ten.sql\")\n",
    "write_query_to_file(QUERY_5, \"trip_count_windiest.sql\")\n",
    "write_query_to_file(QUERY_6, \"Hurricane_Sandy.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
